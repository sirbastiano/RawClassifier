{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchmetrics.classification.matthews_corrcoef import MatthewsCorrCoef\n",
    "import torchmetrics\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import pandas as pd  \n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "import math\n",
    "\n",
    "from flops_counter import get_model_complexity_info\n",
    "from visualisation_utils import plot_csv_values, save_metrics_to_csv, plot_all_data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using {torch.cuda.get_device_name()} for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU for training.\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')  # For performance\n",
    "seed = pl.seed_everything(42, workers=True) # For reproducibility\n",
    "\n",
    "# Initialize a CSVLogger\n",
    "home = '/home/roberto/PythonProjects/S2RAWVessel/mmdetection/data/Venus/classification'\n",
    "csv_logger = CSVLogger('.', name='lightning_logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_lite_params = {\n",
    "    # width_coefficient, depth_coefficient, image_size, dropout_rate\n",
    "    'efficientnet_lite0_venus': [1.0, 1.0, 128, 0.3],\n",
    "    'efficientnet_lite0': [1.0, 1.0, 224, 0.2],\n",
    "    'efficientnet_lite1': [1.0, 1.1, 240, 0.2],\n",
    "    'efficientnet_lite2': [1.1, 1.2, 260, 0.3],\n",
    "    'efficientnet_lite3': [1.2, 1.4, 280, 0.3],\n",
    "    'efficientnet_lite4': [1.4, 1.8, 300, 0.3],\n",
    "}\n",
    "\n",
    "def round_filters(filters, multiplier, divisor=8, min_width=None):\n",
    "    \"\"\"Calculate and round number of filters based on width multiplier.\"\"\"\n",
    "    if not multiplier:\n",
    "        return filters\n",
    "    filters *= multiplier\n",
    "    min_width = min_width or divisor\n",
    "    new_filters = max(min_width, int(filters + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_filters < 0.9 * filters:\n",
    "        new_filters += divisor\n",
    "    return int(new_filters)\n",
    "\n",
    "def round_repeats(repeats, multiplier):\n",
    "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    return int(math.ceil(multiplier * repeats))\n",
    "\n",
    "def drop_connect(x, drop_connect_rate, training):\n",
    "    if not training:\n",
    "        return x\n",
    "    keep_prob = 1.0 - drop_connect_rate\n",
    "    batch_size = x.shape[0]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=x.dtype, device=x.device)\n",
    "    binary_mask = torch.floor(random_tensor)\n",
    "    x = (x / keep_prob) * binary_mask\n",
    "    return x\n",
    "\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, inp, final_oup, k, s, expand_ratio, se_ratio, has_se=False):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "\n",
    "        self._momentum = 0.01\n",
    "        self._epsilon = 1e-3\n",
    "        self.input_filters = inp\n",
    "        self.output_filters = final_oup\n",
    "        self.stride = s\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.has_se = has_se\n",
    "        self.id_skip = True  # skip connection and drop connect\n",
    "\n",
    "        # Expansion phase\n",
    "        oup = inp * expand_ratio  # number of output channels\n",
    "        if expand_ratio != 1:\n",
    "            self._expand_conv = nn.Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._momentum, eps=self._epsilon)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        self._depthwise_conv = nn.Conv2d(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
    "            kernel_size=k, padding=(k - 1) // 2, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._momentum, eps=self._epsilon)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1, int(inp * se_ratio))\n",
    "            self._se_reduce = nn.Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = nn.Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Output phase\n",
    "        self._project_conv = nn.Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._momentum, eps=self._epsilon)\n",
    "        self._relu = nn.ReLU6(inplace=True)\n",
    "\n",
    "    def forward(self, x, drop_connect_rate=None):\n",
    "        \"\"\"\n",
    "        :param x: input tensor\n",
    "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
    "        :return: output of block\n",
    "        \"\"\"\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        identity = x\n",
    "        if self.expand_ratio != 1:\n",
    "            x = self._relu(self._bn0(self._expand_conv(x)))\n",
    "        x = self._relu(self._bn1(self._depthwise_conv(x)))\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(self._relu(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        if self.id_skip and self.stride == 1  and self.input_filters == self.output_filters:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, drop_connect_rate, training=self.training)\n",
    "            x += identity  # skip connection\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientNetLite(nn.Module):\n",
    "    def __init__(self, widthi_multiplier, depth_multiplier, num_classes, drop_connect_rate, dropout_rate):\n",
    "        super(EfficientNetLite, self).__init__()\n",
    "\n",
    "        # Batch norm parameters\n",
    "        momentum = 0.01\n",
    "        epsilon = 1e-3\n",
    "        self.drop_connect_rate = drop_connect_rate\n",
    "        input_channels = 3  # RGB\n",
    "        input_channels = 12 # Spectral bands of Venus\n",
    "        \n",
    "        \n",
    "        mb_block_settings = [\n",
    "            #repeat|kernal_size|stride|expand|input|output|se_ratio\n",
    "                [1, 3, 1, 1, 32,  16,  0.25],\n",
    "                [2, 3, 2, 6, 16,  24,  0.25],\n",
    "                [2, 5, 2, 6, 24,  40,  0.25],\n",
    "                [3, 3, 2, 6, 40,  80,  0.25],\n",
    "                [3, 5, 1, 6, 80,  112, 0.25],\n",
    "                [4, 5, 2, 6, 112, 192, 0.25],\n",
    "                [1, 3, 1, 6, 192, 320, 0.25]\n",
    "            ]\n",
    "\n",
    "        # Stem\n",
    "        out_channels = 32\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Build blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for i, stage_setting in enumerate(mb_block_settings):\n",
    "            stage = nn.ModuleList([])\n",
    "            num_repeat, kernal_size, stride, expand_ratio, input_filters, output_filters, se_ratio = stage_setting\n",
    "            # Update block input and output filters based on width multiplier.\n",
    "            input_filters = input_filters if i == 0 else round_filters(input_filters, widthi_multiplier)\n",
    "            output_filters = round_filters(output_filters, widthi_multiplier)\n",
    "            num_repeat= num_repeat if i == 0 or i == len(mb_block_settings) - 1  else round_repeats(num_repeat, depth_multiplier)\n",
    "            \n",
    "\n",
    "            # The first block needs to take care of stride and filter size increase.\n",
    "            stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
    "            if num_repeat > 1:\n",
    "                input_filters = output_filters\n",
    "                stride = 1\n",
    "            for _ in range(num_repeat - 1):\n",
    "                stage.append(MBConvBlock(input_filters, output_filters, kernal_size, stride, expand_ratio, se_ratio, has_se=False))\n",
    "            \n",
    "            self.blocks.append(stage)\n",
    "\n",
    "        # Head\n",
    "        in_channels = round_filters(mb_block_settings[-1][5], widthi_multiplier)\n",
    "        out_channels = 1280\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(num_features=out_channels, momentum=momentum, eps=epsilon),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        self.fc = torch.nn.Linear(out_channels, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        idx = 0\n",
    "        for stage in self.blocks:\n",
    "            for block in stage:\n",
    "                drop_connect_rate = self.drop_connect_rate\n",
    "                if drop_connect_rate:\n",
    "                    drop_connect_rate *= float(idx) / len(self.blocks)\n",
    "                x = block(x, drop_connect_rate)\n",
    "                idx +=1\n",
    "        x = self.head(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 1.0/float(n))\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def load_pretrain(self, path):\n",
    "        state_dict = torch.load(path)\n",
    "        self.load_state_dict(state_dict, strict=True)\n",
    "        \n",
    "\n",
    "def build_efficientnet_lite(name, num_classes):\n",
    "    width_coefficient, depth_coefficient, _, dropout_rate = efficientnet_lite_params[name]\n",
    "    model = EfficientNetLite(width_coefficient, depth_coefficient, num_classes, 0.2, dropout_rate)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model dimensions and number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'efficientnet_lite0_venus'\n",
    "\n",
    "categories = [x.name for x in Path(\".\").glob(\"*\") if (x.is_dir()) and ('pycache' not in x.name)]\n",
    "\n",
    "model = build_efficientnet_lite(model_name, len(categories))\n",
    "model.eval()\n",
    "\n",
    "wh = efficientnet_lite_params[model_name][2]\n",
    "input_shape = (12, wh, wh)\n",
    "flops, params = get_model_complexity_info(model, input_shape)\n",
    "split_line = '=' * 30\n",
    "print(f'{split_line}\\nInput shape: {input_shape}\\n'\n",
    "        f'Flops: {flops}\\nParams: {params}\\n{split_line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use PyTorch Lightning for Training\n",
    "class StratifiedImageDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        # Ensure the custom loader is passed to the super class\n",
    "        super().__init__(root, loader=self.loader, transform=transform)\n",
    "        self.filepaths = self.samples\n",
    "\n",
    "    @staticmethod\n",
    "    def loader(x):\n",
    "        try:\n",
    "            # Open the image file using rasterio\n",
    "            with rio.open(x) as src:\n",
    "                # Read the data\n",
    "                image_data = src.read()  # shape (channels, height, width)\n",
    "            \n",
    "            # Convert the NumPy array to a PyTorch tensor\n",
    "            tensor = torch.from_numpy(image_data).float()\n",
    "            # Normalize the tensor to 0-1 range if it's a 16-bit image\n",
    "            if tensor.max() > 1.0:\n",
    "                tensor /= 65535\n",
    "\n",
    "            return tensor\n",
    "        except rio.RasterioIOError as e:\n",
    "            # Handle exceptions raised by rasterio\n",
    "            print(f\"RasterioIOError: Could not open {x}: {e}\")\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            # Handle any other exceptions\n",
    "            print(f\"Unexpected error occurred: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.filepaths[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, target\n",
    "\n",
    "class SVenusClassifier(pl.LightningModule):\n",
    "    def __init__(self, image_dir='.', batch_size=32, lr=1e-3,image_size=(128, 128), train_split=0.5, val_split=0.3):\n",
    "        super().__init__()\n",
    "        self.num_workers = 7\n",
    "\n",
    "        model_name = 'efficientnet_lite0_venus'\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        # self.model = timm.create_model(model_name, pretrained=True)\n",
    "        categories = [x.name for x in Path(image_dir).glob(\"*\") if (x.is_dir()) and ('pycache' not in x.name)]\n",
    "        self.num_classes = len(categories)\n",
    "        self.model = build_efficientnet_lite(model_name, self.num_classes)\n",
    "        # dataset: \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size, antialias=True),\n",
    "            # Include any other transformations and normalization here\n",
    "        ])\n",
    "        \n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.lr = lr # learning rate\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        # metrics:\n",
    "        self.precision = torchmetrics.Precision(num_classes=self.num_classes, average='macro')\n",
    "        self.recall = torchmetrics.Recall(num_classes=self.num_classes, average='macro')\n",
    "        self.f1_score = torchmetrics.F1(num_classes=self.num_classes, average='macro')\n",
    "        # self.confmat = torchmetrics.ConfusionMatrix(num_classes=self.num_classes)\n",
    "        self.cohen_kappa = torchmetrics.CohenKappa(num_classes=self.num_classes)\n",
    "        self.balanced_accuracy = torchmetrics.Accuracy(average='macro', num_classes=self.num_classes)\n",
    "        # self.prompt_mcc = MatthewsCorrCoef(num_classes=self.num_classes)\n",
    "        # Instantiate the split\n",
    "        self.setup()\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        # Load the entire dataset\n",
    "        full_dataset = StratifiedImageDataset(root=self.image_dir, transform=self.transform)\n",
    "\n",
    "        # Create a list to hold the indices for stratified split\n",
    "        train_indices, val_indices, test_indices = [], [], []\n",
    "\n",
    "        # Get the list of classes and the number of classes\n",
    "        classes = full_dataset.classes\n",
    "        class_to_idx = full_dataset.class_to_idx\n",
    "\n",
    "        # Perform stratified split\n",
    "        for class_name in classes:\n",
    "            # Get all file indices for the current class\n",
    "            class_indices = [i for i, (_, class_id) in enumerate(full_dataset.samples) if class_id == class_to_idx[class_name]]\n",
    "\n",
    "            # Calculate split sizes for the current class\n",
    "            train_size = int(self.train_split * len(class_indices))\n",
    "            val_size = int(self.val_split * len(class_indices))\n",
    "            test_size = len(class_indices) - train_size - val_size\n",
    "\n",
    "            # Perform the split\n",
    "            class_train_indices, temp_indices = train_test_split(class_indices, train_size=train_size, stratify=None)\n",
    "            class_val_indices, class_test_indices = train_test_split(temp_indices, test_size=test_size, stratify=None)\n",
    "\n",
    "            # Add the class split indices to the respective dataset lists\n",
    "            train_indices.extend(class_train_indices)\n",
    "            val_indices.extend(class_val_indices)\n",
    "            test_indices.extend(class_test_indices)\n",
    "\n",
    "        # Create subsets for each dataset\n",
    "        self.train_dataset = Subset(full_dataset, train_indices)\n",
    "        self.val_dataset = Subset(full_dataset, val_indices)\n",
    "        self.test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, num_workers=self.num_workers)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n",
    "        return {\n",
    "                    'optimizer': optimizer,\n",
    "                    'lr_scheduler': scheduler,\n",
    "                    'monitor': 'cohen_kappa',  # optional key used for early stopping\n",
    "                }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "\n",
    "        # Update metrics\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        metrics = {'train_loss': loss,'train_acc':acc, 'lr': self.lr}\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        # Update metrics\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.precision.update(preds, y)\n",
    "        self.recall.update(preds, y)\n",
    "        self.f1_score.update(preds, y)\n",
    "        self.cohen_kappa.update(preds, y)\n",
    "        self.balanced_accuracy.update(preds, y)\n",
    "\n",
    "        metrics = {'val_loss': loss, 'val_acc': acc,'lr': self.lr, 'precision': self.precision, 'recall': self.recall, 'f1_score': self.f1_score, 'cohen_kappa': self.cohen_kappa, 'balanced_accuracy': self.balanced_accuracy}\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.precision.update(preds, y)\n",
    "        self.recall.update(preds, y)\n",
    "        self.f1_score.update(preds, y)\n",
    "        self.cohen_kappa.update(preds, y)\n",
    "        self.balanced_accuracy.update(preds, y)\n",
    "        \n",
    "        metrics = {'test_loss': loss, 'test_acc': acc, 'test_precision': self.precision, 'test_recall': self.recall, 'test_f1_score': self.f1_score, 'test_cohen_kappa': self.cohen_kappa, 'test_balanced_accuracy': self.balanced_accuracy}\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.test_results = metrics\n",
    "        return metrics\n",
    "    \n",
    "    # def on_validation_epoch_end(self):\n",
    "\n",
    "    # def on_test_epoch_end(self):\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS = 32\n",
    "# LR = 1e-3\n",
    "MAX_EPOCHS = 30\n",
    "\n",
    "for BS in [8, 12]:\n",
    "    for LR in [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2]:\n",
    "        venus_classifier = SVenusClassifier(image_dir=f'{home}/V2C', batch_size=BS, image_size=(128, 128), train_split=0.5, val_split=0.3, lr=LR)\n",
    "        trainer = pl.Trainer(max_epochs=MAX_EPOCHS, log_every_n_steps=10, logger=csv_logger)\n",
    "        trainer.fit(venus_classifier)\n",
    "        test_results = trainer.test(venus_classifier)\n",
    "        save_metrics_to_csv(test_results, f'{home}/test_results/venus_classifier_{BS}_{LR}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
